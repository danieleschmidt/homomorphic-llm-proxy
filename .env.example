# FHE LLM Proxy Environment Configuration
# Copy this file to .env and configure your settings

# Server Configuration
RUST_LOG=info
HOST=0.0.0.0
PORT=8080
METRICS_PORT=9090

# LLM Provider Configuration
LLM_ENDPOINT=https://api.openai.com/v1
LLM_API_KEY=your_openai_api_key_here
LLM_TIMEOUT=300
LLM_MAX_RETRIES=3

# Redis Configuration
REDIS_URL=redis://localhost:6379
REDIS_PASSWORD=
REDIS_DB=0

# Encryption Configuration
POLY_MODULUS_DEGREE=16384
COEFF_MODULUS_BITS=60,40,40,60
SCALE_BITS=40

# GPU Configuration
GPU_DEVICE_ID=0
GPU_BATCH_SIZE=32
GPU_KERNEL_OPTIMIZATION=aggressive

# Privacy Configuration
EPSILON_PER_QUERY=0.1
DELTA=1e-5
MAX_QUERIES_PER_USER=1000

# Security Configuration
KEY_ROTATION_HOURS=24
SECURITY_LEVEL=128

# Monitoring Configuration
GRAFANA_PASSWORD=admin
TRACE_SAMPLING_RATE=0.1

# Development Configuration
DEV_MODE=false
DEBUG_FHE=false
MOCK_GPU=false